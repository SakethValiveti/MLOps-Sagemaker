{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Environment for BIA Pipeline\n",
    "\n",
    "This notebook instance will act as the lab environment for setting up and triggering changes to our pipeline.  This is being used to provide a consistent environment, gain some familiarity with Amazon SageMaker Notebook Instances, and to avoid any issues with debugging individual laptop configurations during the workshop. \n",
    "\n",
    "PLEASE review the sample notebook [xgboost_customer_churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn) for detailed documentation on the model being built\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  View the Data\n",
    "\n",
    "In this step we are going to upload the data that was processed using the same processing detailed in the  example notebook, [xgboost_customer_churn](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_applying_machine_learning/xgboost_customer_churn).  \n",
    "\n",
    "The following sample shows the label and a subset of the features included in the training dataset.  The label is in the first column, churn, which is the value we are trying to predict to determine whether a customer will churn. \n",
    "\n",
    "[Sample with Header](images/training_data_sample.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data\n",
      "       0  106  0.1  274.4  120  198.6   82  160.8   62   6.0  3  1  0.2  0.3  \\\n",
      "0     0   28    0  187.8   94  248.6   86  208.8  124  10.6  5  0    0    0   \n",
      "1     1  148    0  279.3  104  201.6   87  280.8   99   7.9  2  2    0    0   \n",
      "...  ..  ...  ...    ...  ...    ...  ...    ...  ...   ... .. ..  ...  ...   \n",
      "2330  0  159    0  198.8  107  195.5   91  213.3  120  16.5  7  5    0    0   \n",
      "2331  0   99   33  179.1   93  238.3  102  165.7   96  10.6  1  2    0    0   \n",
      "\n",
      "      0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  1.1  0.15  \\\n",
      "0       0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "1       0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "...   ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "2330    0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "2331    0    0    0    0    0    0     0     0     0     0     0    0     0   \n",
      "\n",
      "      0.16  0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  0.27  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2331     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.28  0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  0.39  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1        0     0     0     0     0     0     1     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     1     0     0   \n",
      "2331     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.40  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  0.51  \\\n",
      "0        0     0     0     0     0     0     0     0     0     0     0     1   \n",
      "1        0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "2330     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2331     0     1     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "      0.52  0.53  1.2  1.3  0.54  1.4  0.55  \n",
      "0        0     1    0    1     0    1     0  \n",
      "1        0     1    0    1     0    1     0  \n",
      "...    ...   ...  ...  ...   ...  ...   ...  \n",
      "2330     0     0    1    1     0    1     0  \n",
      "2331     0     1    0    1     0    0     1  \n",
      "\n",
      "[2332 rows x 70 columns]\n",
      "\n",
      "Smoke Test Data\n",
      "    0  138  0.1  127.1  102  247.7  106  207.7   75   5.0  3  3.1  0.2  0.3  \\\n",
      "0  0  120    0  252.0  120  150.2  106  151.8   96   9.6  1    2    0    0   \n",
      "1  0  112   30   60.6  113  165.9   96  132.8   99  13.3  7    0    0    0   \n",
      "2  0   70    0  197.3   91  305.8   81  171.0  105   6.7  6    1    0    0   \n",
      "3  0   81   46  168.3  124  270.9  103  222.5   98   6.7  2    4    0    0   \n",
      "\n",
      "   0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  0.15  0.16  \\\n",
      "0    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "1    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "2    0    0    0    0    0    0     0     0     0     1     0     0     0   \n",
      "3    0    0    0    0    0    0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  0.27  0.28  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     1     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  0.39  0.40  \\\n",
      "0     0     0     0     0     0     0     0     0     0     0     1     0   \n",
      "1     0     0     0     0     0     0     0     1     0     0     0     0   \n",
      "2     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "   0.41  0.42  1  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  0.51  0.52  \\\n",
      "0     0     0  0     0     0     0     0     0     0     0     0     0     0   \n",
      "1     0     0  0     0     0     0     0     0     0     0     0     0     1   \n",
      "2     0     0  0     0     0     0     0     0     0     0     0     0     0   \n",
      "3     0     0  0     0     0     0     0     0     0     0     0     0     1   \n",
      "\n",
      "   0.53  1.1  1.2  0.54  1.3  0.55  \n",
      "0     1    0    1     0    1     0  \n",
      "1     0    0    1     0    0     1  \n",
      "2     1    0    1     0    1     0  \n",
      "3     0    0    1     0    0     1  \n",
      "\n",
      "Validation Data\n",
      "        0   47  28  141.3   94  168.0  108  113.5   84   7.8  2  1  0.1  1.1  \\\n",
      "0    0.0   30   0  247.4  107  175.9   76  287.4   90  11.3  2  0    0    0   \n",
      "1    0.0  106  32  165.9  126  216.5   93  173.1   86  14.1  8  4    0    0   \n",
      "..   ...  ...  ..    ...  ...    ...  ...    ...  ...   ... .. ..  ...  ...   \n",
      "663  0.0   70   0  197.3   91  305.8   81  171.0  105   6.7  6  1    0    0   \n",
      "664  0.0   81  46  168.3  124  270.9  103  222.5   98   6.7  2  4    0    0   \n",
      "\n",
      "     0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  0.10  0.11  0.12  0.13  0.14  \\\n",
      "0      0    0    0    0    0    0    0    0     0     0     0     0     0   \n",
      "1      0    0    0    0    0    0    0    1     0     0     0     0     0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...   \n",
      "663    0    0    0    0    0    0    0    0     0     1     0     0     0   \n",
      "664    0    0    0    0    0    0    0    0     0     0     0     0     0   \n",
      "\n",
      "     0.15  0.16  0.17  0.18  0.19  0.20  0.21  0.22  0.23  0.24  0.25  0.26  \\\n",
      "0       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     1     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     0.27  0.28  0.29  0.30  0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  \\\n",
      "0       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     0.39  0.40  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.50  \\\n",
      "0       0     0     1     0     0     0     0     0     0     0     0     0   \n",
      "1       0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "663     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "664     0     0     0     0     0     0     0     0     0     0     0     0   \n",
      "\n",
      "     1.2  0.51  0.52  1.3  0.53  0.54  1.4  \n",
      "0      0     1     0    1     0     1    0  \n",
      "1      1     0     0    1     0     0    1  \n",
      "..   ...   ...   ...  ...   ...   ...  ...  \n",
      "663    0     1     0    1     0     1    0  \n",
      "664    1     0     0    1     0     0    1  \n",
      "\n",
      "[665 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('./data/train.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nTraining Data\\n', train_data)\n",
    "\n",
    "smoketest_data = pd.read_csv('./data/smoketest.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nSmoke Test Data\\n', smoketest_data)\n",
    "\n",
    "validation_data = pd.read_csv('./data/validation.csv', sep=',')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 5)         # Keep the output on one page\n",
    "print('\\nValidation Data\\n', validation_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2:  Upload Data to S3 \n",
    "\n",
    "We will utilize this notebook to perform some of the setup that will be required to trigger the first execution of our pipeline.  In this step, we are going to simulate what would typically be the last step in an Analytics pipeline of creating training and validation datasets. \n",
    "\n",
    "To accomplish this, we will actually be uploading data from our local notebook instance (data can be found under /data/*) to S3.  In a typical scenario, this would be done through your analytics pipeline.  We will use the S3 bucket that was created through the CloudFormation template we launched at the beginning of the lab. You can validate the S3 bucket exists by:\n",
    "  1. Going to the [S3 Service](https://s3.console.aws.amazon.com/s3/) inside the AWS Console\n",
    "  2. Find the name of the S3 data bucket created by the CloudFormation template: mlops-bia- data-*yourintials*-*randomid*\n",
    "  3. Update the bucket variable in the cell below\n",
    "\n",
    "   ### UPDATE THE BUCKET NAME BELOW BEFORE EXECUTING THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import time\n",
    "\n",
    "# UPDATE THE NAME OF THE BUCKET TO MATCH THE ONE WE CREATED THROUGH THE CLOUDFORMATION TEMPLATE\n",
    "# Example: mlops-bia-data-jdd-df4d4850\n",
    "#bucket = 'mlops-bia-data-<yourinitials>-<generated id>'\n",
    "bucket = 'mlops-bia-data-yourintials-uniqueid'\n",
    "\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "\n",
    "trainfilename = 'train/train.csv'\n",
    "smoketestfilename = 'smoketest/smoketest.csv'\n",
    "validationfilename = 'validation/validation.csv'\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.meta.client.upload_file('./data/train.csv', bucket, trainfilename)\n",
    "s3.meta.client.upload_file('./data/smoketest.csv', bucket, smoketestfilename)\n",
    "s3.meta.client.upload_file('./data/validation.csv', bucket, validationfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 3:  Monitor CodePipeline Execution\n",
    "\n",
    "The code above will trigger the execution of your CodePipeline. You can monitor progress of the pipeline execution in the [CodePipeline dashboard](https://console.aws.amazon.com/codesuite/codepipeline/pipelines).  Within the pipeline, explore the stages while the pipeline is execution to understand what is being performed and what user pararameters are being included as input into each stage.   For example, StartTraining takes a set of parameters detailing the training environment as well as the hyperparameters for training: \n",
    "\n",
    "    {\"Algorithm\": \"xgboost:1\", \"traincompute\": \"ml.c4.2xlarge\" , \"traininstancevolumesize\": 10, \"traininstancecount\": 1, \"MaxDepth\": \"5\", \"eta\": \"0.2\", \"gamma\": \"4\", \"MinChildWeight\": \"6\", \"SubSample\": 0.8, \"Silent\": 0, \"Objective\": \"binary:logistic\", \"NumRound\": \"100\"} \n",
    "\n",
    "As the pipeline is executing information is being logged to [CloudWatch logs](https://console.aws.amazon.com/cloudwatch/logs).  Explore the logs for your Lambda functions (/aws/lambda/MLOps-BIA*) as well as output logs from SageMaker (/aws/sagemaker/*).  Also, since this is a Built-In Algorithm, SageMaker automatically emits training metrics to understand how well your model is learning and whether it will generalize well on unseen data. Those metrics are logged to /aws/sagemaker/TrainingJobs in CloudWatch\n",
    "\n",
    "\n",
    "Note: It will take awhile to execute all the way through the pipeline.  Please don't proceed to the next step until the last stage is shows **'succeeded'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Additional Clean-Up\n",
    "\n",
    "Return to the [README.md](https://github.com/aws-samples/amazon-sagemaker-devops-with-ml/1-Built-In-Algorithm/README.md) to complete the environment cleanup instructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONGRATULATIONS! \n",
    "\n",
    "You've built a basic pipeline for the use case of utilizing a Built-In SageMaker algorithm.  This pipeline can act as a starting point for building in additional capabilities such as additional quality gates, more dynamic logic for capturing hyperparameter changes, various deployment strategies (ex. A/B Testing).  Another common extension to the pipeline may be creating/updating your API serving predictions through API Gateway.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
